{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6go_zLrDlhZ"
      },
      "source": [
        "# Complete Code: Bayesian PASA vs Baselines on CIFAR-10-C (Colab Ready)\n",
        "\n",
        "# This notebook implements a full comparison between the proposed Bayesian PASA activation and standard baselines (ReLU, LeakyReLU, GELU, Swish, Mish) on the CIFAR-10-C corrupted dataset. It integrates the Bayesian R‚ÄëLayerNorm normalization and the new Bayesian PASA activation, providing:\n",
        "\n",
        "  Extraction of CIFAR-10-C from a zip file on Google Drive (or fallback to on‚Äëthe‚Äëfly corruption).\n",
        "\n",
        "  Definitions of all activations (ReLU, LeakyReLU, GELU, Swish, Mish, original PASA, Bayesian PASA).\n",
        "\n",
        "  Definitions of normalization layers (Standard LayerNorm, R‚ÄëLayerNorm, Bayesian R‚ÄëLayerNorm).\n",
        "\n",
        "  A flexible CNN (EfficientCNN) that can combine any activation and normalization.\n",
        "\n",
        "  Training and evaluation on four corruption types (Gaussian, Shot, Blur, Contrast).\n",
        "\n",
        "  Collection of softmax weights from Bayesian PASA for stability analysis.\n",
        "\n",
        "  Generation of comparison plots and final tables.\n",
        "\n",
        "The code is optimized for Google Colab with a T4 GPU and 12GB RAM. All memory is managed to avoid OOM errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYpll1MVDmBh"
      },
      "source": [
        "üöÄ Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzUnufWYDvsB",
        "outputId": "2ab42b2f-3a48-473c-d21b-3df98417c05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "VRAM: 15.6 GB\n",
            "Using on-the-fly corruption generation (no Drive mounting required)\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Bayesian_PASA_vs_Baselines_CIFAR10C.ipynb\n",
        "\n",
        "Automatically generated for Colab T4 GPU.\n",
        "\"\"\"\n",
        "\n",
        "# Install required packages (if not already installed)\n",
        "!pip install torch torchvision tqdm matplotlib seaborn fpdf --quiet\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "from PIL import Image, ImageFilter\n",
        "import gc\n",
        "from copy import deepcopy\n",
        "\n",
        "# For PDF report generation (optional)\n",
        "from fpdf import FPDF\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Non-interactive backend to save memory\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# Skip Drive mounting entirely - use on-the-fly corruption\n",
        "print(\"Using on-the-fly corruption generation (no Drive mounting required)\")\n",
        "use_precomputed_cifar10c = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXKQxOnemq6L"
      },
      "source": [
        "üß† Activation Functions\n",
        "\n",
        "Original PASA (simplified stable version with running stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nMnhfxWDmbMj"
      },
      "outputs": [],
      "source": [
        "class PASA(nn.Module):\n",
        "    \"\"\"\n",
        "    Original PASA ‚Äì Stable version with running averages and learnable temperature.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 alpha0=1.0, alpha1=0.5, kappa=0.1,\n",
        "                 tau=5.0, beta=1.0,\n",
        "                 lambda1=0.5, mu1=0.0, tau_lin=2.0,\n",
        "                 eps=1e-6, momentum=0.99,\n",
        "                 temperature_init=1.0):\n",
        "        super().__init__()\n",
        "        self.alpha0 = nn.Parameter(torch.tensor(alpha0))\n",
        "        self.alpha1 = nn.Parameter(torch.tensor(alpha1))\n",
        "        self.kappa  = nn.Parameter(torch.tensor(kappa))\n",
        "        self.tau_mix = nn.Parameter(torch.tensor(temperature_init))\n",
        "\n",
        "        self.tau = tau\n",
        "        self.beta = beta\n",
        "        self.lambda1 = lambda1\n",
        "        self.mu1 = mu1\n",
        "        self.tau_lin = tau_lin\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Running statistics\n",
        "        self.register_buffer('running_absmean', torch.tensor(0.5))\n",
        "        self.register_buffer('running_noise_var', torch.tensor(1.0))\n",
        "        self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
        "\n",
        "    def forward(self, x, return_weights=False):\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                batch_absmean = x.abs().mean()\n",
        "                batch_var = x.var(unbiased=False)\n",
        "                if self.num_batches_tracked == 0:\n",
        "                    self.running_absmean = batch_absmean\n",
        "                    self.running_noise_var = batch_var\n",
        "                else:\n",
        "                    self.running_absmean = (self.momentum * self.running_absmean +\n",
        "                                            (1 - self.momentum) * batch_absmean)\n",
        "                    self.running_noise_var = (self.momentum * self.running_noise_var +\n",
        "                                              (1 - self.momentum) * batch_var)\n",
        "                self.num_batches_tracked += 1\n",
        "\n",
        "        mu_abs = self.running_absmean\n",
        "        sigma2 = self.running_noise_var.clamp(min=1e-2, max=1e2)\n",
        "        log_sigma2 = torch.log(sigma2).clamp(min=-10, max=10)\n",
        "\n",
        "        # Component functions\n",
        "        alpha = self.alpha0 + self.alpha1 * torch.tanh(self.kappa * mu_abs)\n",
        "        S = torch.sigmoid(alpha * x)\n",
        "        L = x / (1 + x.abs() / self.tau)\n",
        "        sigma = torch.sqrt(sigma2)\n",
        "        z = x / (self.beta * sigma * 1.41421356237)\n",
        "        N = torch.tanh(1.4 * z)\n",
        "\n",
        "        # Evidence scores\n",
        "        log_prior = torch.log(torch.tensor(1/3.0, device=x.device))\n",
        "        E1 = -self.lambda1 * (x - self.mu1)**2 + log_prior\n",
        "        E2 = -x.abs() / self.tau_lin + log_prior\n",
        "        E3 = - (x**2) / (2 * sigma2) - log_sigma2 + log_prior\n",
        "\n",
        "        # Softmax mixing with temperature\n",
        "        w = torch.softmax(torch.stack([E1, E2, E3], dim=-1) / self.tau_mix, dim=-1)\n",
        "\n",
        "        out = w[..., 0] * S + w[..., 1] * L + w[..., 2] * N\n",
        "        if return_weights:\n",
        "            return out, w\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GQBQLismt7L"
      },
      "source": [
        "Bayesian PASA (new formulation with œà‚Äëfunction and variational evidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uOl6kdzzmcBj"
      },
      "outputs": [],
      "source": [
        "class BayesianPASA(nn.Module):\n",
        "    \"\"\"\n",
        "    Bayesian PASA ‚Äì Incorporates œà‚Äëfunction and variational evidence.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 alpha0=1.0, alpha1=0.5, kappa=0.1,\n",
        "                 tau=5.0, beta=1.0,\n",
        "                 lambda1=0.5, mu1=0.0, tau_lin=2.0,\n",
        "                 lambda3=0.1,                # noise branch regularization\n",
        "                 eps=1e-6, momentum=0.99,\n",
        "                 temperature_init=1.0):\n",
        "        super().__init__()\n",
        "        self.alpha0 = nn.Parameter(torch.tensor(alpha0))\n",
        "        self.alpha1 = nn.Parameter(torch.tensor(alpha1))\n",
        "        self.kappa  = nn.Parameter(torch.tensor(kappa))\n",
        "        self.tau_mix = nn.Parameter(torch.tensor(temperature_init))\n",
        "\n",
        "        self.tau = tau\n",
        "        self.beta = beta\n",
        "        self.lambda1 = lambda1\n",
        "        self.mu1 = mu1\n",
        "        self.tau_lin = tau_lin\n",
        "        self.lambda3 = lambda3\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Running statistics\n",
        "        self.register_buffer('running_absmean', torch.tensor(0.5))\n",
        "        self.register_buffer('running_noise_var', torch.tensor(1.0))\n",
        "        self.register_buffer('running_local_entropy', torch.tensor(1.0))\n",
        "        self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
        "\n",
        "    def psi(self, t):\n",
        "        \"\"\"Stable œà(t) = log(1+t) - t/(1+t)\"\"\"\n",
        "        return torch.log1p(t) - t / (1 + t)\n",
        "\n",
        "    def forward(self, x, return_weights=False):\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # Update running statistics during training\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                # Global stats\n",
        "                batch_absmean = x.abs().mean()\n",
        "                batch_var = x.var(unbiased=False)\n",
        "\n",
        "                # Local entropy estimate (using 3x3 patches)\n",
        "                x_padded = F.pad(x, (1,1,1,1), mode='reflect')\n",
        "                local_mean = F.avg_pool2d(x_padded, kernel_size=3, stride=1)\n",
        "                local_var = F.avg_pool2d(x_padded**2, kernel_size=3, stride=1) - local_mean**2\n",
        "                local_entropy = local_var.mean(dim=[1,2,3]).mean()  # scalar per batch\n",
        "\n",
        "                if self.num_batches_tracked == 0:\n",
        "                    self.running_absmean = batch_absmean\n",
        "                    self.running_noise_var = batch_var\n",
        "                    self.running_local_entropy = local_entropy\n",
        "                else:\n",
        "                    self.running_absmean = (self.momentum * self.running_absmean +\n",
        "                                            (1 - self.momentum) * batch_absmean)\n",
        "                    self.running_noise_var = (self.momentum * self.running_noise_var +\n",
        "                                              (1 - self.momentum) * batch_var)\n",
        "                    self.running_local_entropy = (self.momentum * self.running_local_entropy +\n",
        "                                                  (1 - self.momentum) * local_entropy)\n",
        "                self.num_batches_tracked += 1\n",
        "\n",
        "        # Use running averages\n",
        "        mu_abs = self.running_absmean\n",
        "        sigma2 = self.running_noise_var.clamp(min=1e-2, max=1e2)\n",
        "        log_sigma2 = torch.log(sigma2).clamp(min=-10, max=10)\n",
        "        local_E = self.running_local_entropy.clamp(min=1e-2)\n",
        "\n",
        "        # Component functions with œà modulation\n",
        "        # Adaptive sigmoid slope\n",
        "        alpha_slope = self.alpha0 + self.alpha1 * torch.tanh(self.kappa * self.psi(self.lambda3 * local_E))\n",
        "        S = torch.sigmoid(alpha_slope * x)\n",
        "\n",
        "        # Moderate linear (unchanged)\n",
        "        L = x / (1 + x.abs() / self.tau)\n",
        "\n",
        "        # Noise‚Äëaware branch with effective sigma\n",
        "        sigma_eff = torch.sqrt(sigma2) * torch.exp(0.5 * self.psi(self.lambda3 * local_E))\n",
        "        z = x / (self.beta * sigma_eff * 1.41421356237)\n",
        "        N = torch.tanh(1.4 * z)\n",
        "\n",
        "        # Evidence scores derived from variational approximation\n",
        "        log_prior = torch.log(torch.tensor(1/3.0, device=x.device))\n",
        "        E1 = -0.5 * self.lambda1 * (x - self.mu1)**2 + log_prior\n",
        "        E2 = -x.abs() / self.tau_lin + log_prior\n",
        "        E3 = -0.5 * (x**2) / sigma2 - 0.5 * log_sigma2 - 0.5 * self.psi(self.lambda3 * local_E) + log_prior\n",
        "\n",
        "        # Softmax with learnable temperature\n",
        "        w = torch.softmax(torch.stack([E1, E2, E3], dim=-1) / self.tau_mix, dim=-1)\n",
        "\n",
        "        out = w[..., 0] * S + w[..., 1] * L + w[..., 2] * N\n",
        "        if return_weights:\n",
        "            return out, w\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkJdmH9-mx9h"
      },
      "source": [
        "Baseline Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "moMrrfbKmctF"
      },
      "outputs": [],
      "source": [
        "class Mish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def __init__(self, beta=1.0):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(self.beta * x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5jiousxm474"
      },
      "source": [
        "üß™ Normalization Layers (from Bayesian R‚ÄëLayerNorm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GEe2FDT4mdl8"
      },
      "outputs": [],
      "source": [
        "class StandardLayerNorm(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(1, num_features, 1, 1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1, num_features, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=[2,3], keepdim=True)\n",
        "        var = x.var(dim=[2,3], keepdim=True, unbiased=False)\n",
        "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.weight * x_norm + self.bias\n",
        "\n",
        "class RLayerNorm(nn.Module):\n",
        "    def __init__(self, num_features, lambda_init=0.01, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.lambda_param = nn.Parameter(torch.tensor(lambda_init))\n",
        "        self.weight = nn.Parameter(torch.ones(1, num_features, 1, 1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1, num_features, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        mean = x.mean(dim=[2,3], keepdim=True)\n",
        "        var = x.var(dim=[2,3], keepdim=True, unbiased=False)\n",
        "        std = torch.sqrt(var + self.eps)\n",
        "\n",
        "        # Local variance for noise estimate\n",
        "        x_padded = F.pad(x, (1,1,1,1), mode='reflect')\n",
        "        local_means = F.avg_pool2d(x_padded, kernel_size=3, stride=1)\n",
        "        local_vars = F.avg_pool2d(x_padded**2, kernel_size=3, stride=1) - local_means**2\n",
        "        local_vars = local_vars.clamp(min=0)\n",
        "        noise_estimate = local_vars.mean(dim=[2,3], keepdim=True)\n",
        "\n",
        "        lambda_safe = self.lambda_param.clamp(1e-3, 1.0)\n",
        "        noise_scale = 1 + lambda_safe * noise_estimate / (var + self.eps)\n",
        "        x_norm = (x - mean) / (std * noise_scale + self.eps)\n",
        "        return self.weight * x_norm + self.bias\n",
        "\n",
        "class BayesianRLayerNorm(nn.Module):\n",
        "    def __init__(self, num_features, lambda_init=0.01, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.lambda_param = nn.Parameter(torch.tensor(lambda_init))\n",
        "        self.weight = nn.Parameter(torch.ones(1, num_features, 1, 1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1, num_features, 1, 1))\n",
        "\n",
        "    def psi(self, t):\n",
        "        return torch.log1p(t) - t / (1 + t)\n",
        "\n",
        "    def forward(self, x, return_uncertainty=False):\n",
        "        B, C, H, W = x.shape\n",
        "        mean = x.mean(dim=[2,3], keepdim=True)\n",
        "        var = x.var(dim=[2,3], keepdim=True, unbiased=False)\n",
        "        std = torch.sqrt(var + self.eps)\n",
        "\n",
        "        x_padded = F.pad(x, (1,1,1,1), mode='reflect')\n",
        "        local_means = F.avg_pool2d(x_padded, kernel_size=3, stride=1)\n",
        "        local_vars = F.avg_pool2d(x_padded**2, kernel_size=3, stride=1) - local_means**2\n",
        "        local_vars = local_vars.clamp(min=0)\n",
        "        noise_estimate = local_vars.mean(dim=[2,3], keepdim=True)\n",
        "\n",
        "        lambda_safe = self.lambda_param.clamp(1e-3, 1.0)\n",
        "        lambdaE = lambda_safe * noise_estimate / (var + self.eps)\n",
        "        psi_term = self.psi(lambdaE)\n",
        "\n",
        "        effective_std = std * torch.exp(0.5 * psi_term)\n",
        "        normalized = (x - mean) / (effective_std + self.eps)\n",
        "        output = self.weight * normalized + self.bias\n",
        "\n",
        "        if return_uncertainty:\n",
        "            uncertainty = 1.0 / (effective_std**2 + self.eps)\n",
        "            return output, uncertainty\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9A9VA-Hm9uu"
      },
      "source": [
        "üèóÔ∏è Flexible CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TSbEa6kmmeCj"
      },
      "outputs": [],
      "source": [
        "class EfficientCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN with configurable normalization and activation.\n",
        "    \"\"\"\n",
        "    def __init__(self, norm_type='layer', activation='relu', num_classes=10):\n",
        "        super().__init__()\n",
        "        self.norm_type = norm_type\n",
        "        self.activation = activation\n",
        "\n",
        "        # Feature extractor\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.norm1 = self._create_norm_layer(16)\n",
        "        self.act1 = self._create_activation()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.norm2 = self._create_norm_layer(32)\n",
        "        self.act2 = self._create_activation()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.norm3 = self._create_norm_layer(64)\n",
        "        self.act3 = self._create_activation()\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.return_weights = False  # used for PASA analysis\n",
        "\n",
        "    def _create_norm_layer(self, num_features):\n",
        "        if self.norm_type == 'layer':\n",
        "            return StandardLayerNorm(num_features)\n",
        "        elif self.norm_type == 'r_layer':\n",
        "            return RLayerNorm(num_features)\n",
        "        elif self.norm_type == 'bayesian_r_layer':\n",
        "            return BayesianRLayerNorm(num_features)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown norm_type: {self.norm_type}\")\n",
        "\n",
        "    def _create_activation(self):\n",
        "        if self.activation == 'relu':\n",
        "            return nn.ReLU()\n",
        "        elif self.activation == 'leakyrelu':\n",
        "            return nn.LeakyReLU(0.01)\n",
        "        elif self.activation == 'gelu':\n",
        "            return nn.GELU()\n",
        "        elif self.activation == 'swish':\n",
        "            return Swish()\n",
        "        elif self.activation == 'mish':\n",
        "            return Mish()\n",
        "        elif self.activation == 'pasa':\n",
        "            return PASA()\n",
        "        elif self.activation == 'bayesian_pasa':\n",
        "            return BayesianPASA()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation: {self.activation}\")\n",
        "\n",
        "    def forward(self, x, return_weights=False):\n",
        "        self.return_weights = return_weights\n",
        "\n",
        "        # Block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # Block 2\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Block 3 ‚Äì handle weight return for PASA\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm3(x)\n",
        "\n",
        "        if self.return_weights and isinstance(self.act3, (PASA, BayesianPASA)):\n",
        "            x, weights = self.act3(x, return_weights=True)\n",
        "        else:\n",
        "            x = self.act3(x)\n",
        "            weights = None\n",
        "\n",
        "        x = self.pool3(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        if weights is not None:\n",
        "            return x, weights\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjqA7Y4vnDoQ"
      },
      "source": [
        "üì¶ Dataset: CIFAR-10-C (on‚Äëthe‚Äëfly corruption)\n",
        "\n",
        "We use the same CachedNoisyCIFAR10 as in the original code, but we add an option to load pre‚Äëcorrupted images if use_precomputed_cifar10c is True. For simplicity, we'll keep the on‚Äëthe‚Äëfly generation, which is faster and doesn't require a specific folder structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jHM7_wa0me00"
      },
      "outputs": [],
      "source": [
        "class CachedNoisyCIFAR10(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset with in-memory caching for speed.\n",
        "    Applies various noise corruptions to CIFAR-10 images.\n",
        "    \"\"\"\n",
        "    def __init__(self, noise_type='gaussian', severity=3, num_samples=1000, train=True):\n",
        "        self.noise_type = noise_type\n",
        "        self.severity = severity\n",
        "        self.train = train\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "        # Download clean CIFAR-10\n",
        "        transform = transforms.ToTensor()\n",
        "        self.clean_dataset = CIFAR10(root='./data', train=train, download=True, transform=None)\n",
        "\n",
        "        # Cache samples\n",
        "        self.cached_samples = []\n",
        "        self.cached_labels = []\n",
        "\n",
        "        indices = list(range(min(num_samples, len(self.clean_dataset))))\n",
        "        print(f\"Caching {noise_type} samples...\")\n",
        "        for idx in tqdm(indices, leave=False):\n",
        "            img, label = self.clean_dataset[idx]\n",
        "            img_tensor = self.apply_noise_and_transform(img)\n",
        "            self.cached_samples.append(img_tensor)\n",
        "            self.cached_labels.append(label)\n",
        "\n",
        "    def apply_noise_and_transform(self, img_pil):\n",
        "        img_np = np.array(img_pil).astype(np.float32) / 255.0\n",
        "\n",
        "        if self.noise_type == 'gaussian':\n",
        "            noise = np.random.randn(*img_np.shape) * 0.1 * self.severity\n",
        "            img_np = img_np + noise\n",
        "        elif self.noise_type == 'shot_noise':\n",
        "            mask = np.random.random(img_np.shape) < 0.05 * self.severity\n",
        "            salt = np.random.random(mask.sum()) > 0.5\n",
        "            img_np_flat = img_np.reshape(-1)\n",
        "            mask_flat = mask.reshape(-1)\n",
        "            img_np_flat[mask_flat] = salt.astype(np.float32)\n",
        "            img_np = img_np_flat.reshape(img_np.shape)\n",
        "        elif self.noise_type == 'blur':\n",
        "            img_pil = img_pil.filter(ImageFilter.GaussianBlur(radius=self.severity*0.5))\n",
        "            img_np = np.array(img_pil).astype(np.float32) / 255.0\n",
        "        elif self.noise_type == 'contrast':\n",
        "            mean = img_np.mean()\n",
        "            contrast_factor = max(0.5, 1.0 - 0.2 * self.severity)\n",
        "            img_np = contrast_factor * (img_np - mean) + mean\n",
        "\n",
        "        img_np = np.clip(img_np, 0, 1)\n",
        "        img_tensor = torch.from_numpy(img_np.transpose(2, 0, 1)).float()\n",
        "\n",
        "        # Normalize with CIFAR-10 stats\n",
        "        normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "        img_tensor = normalize(img_tensor)\n",
        "        return img_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cached_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.cached_samples[idx], self.cached_labels[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFhzhfEL19In"
      },
      "source": [
        "üèãÔ∏è Training and Evaluation Functions\n",
        "\n",
        "We'll create a function to train a model for a given combination of normalization and activation, and return test accuracies per noise type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_6gGeg-Cmfhg"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(norm_type, activation, num_epochs=5, lr=0.001,\n",
        "                       train_samples=500, test_samples=200):\n",
        "    \"\"\"\n",
        "    Train a model with specified norm and activation on mixed corruptions,\n",
        "    then evaluate on each noise type.\n",
        "    Returns: dict of test accuracies per noise, and the trained model.\n",
        "    \"\"\"\n",
        "    print(f\"\\n>>> Training: norm={norm_type}, act={activation}\")\n",
        "\n",
        "    model = EfficientCNN(norm_type=norm_type, activation=activation).to(device)\n",
        "\n",
        "    # Datasets: mix all noise types for training\n",
        "    noise_types = ['gaussian', 'shot_noise', 'blur', 'contrast']\n",
        "    datasets = []\n",
        "    for noise in noise_types:\n",
        "        ds = CachedNoisyCIFAR10(noise_type=noise, severity=3,\n",
        "                                 num_samples=train_samples, train=True)\n",
        "        datasets.append(ds)\n",
        "    train_loader = DataLoader(ConcatDataset(datasets), batch_size=32,\n",
        "                              shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
        "        for inputs, targets in pbar:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            pbar.set_postfix({'loss': loss.item(), 'acc': 100.*correct/total})\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = 100. * correct / total\n",
        "        print(f\"  Epoch {epoch+1}: Loss={epoch_loss:.4f}, Acc={epoch_acc:.2f}%\")\n",
        "\n",
        "    # Evaluation on each noise type\n",
        "    model.eval()\n",
        "    test_results = {}\n",
        "    for noise in noise_types:\n",
        "        test_ds = CachedNoisyCIFAR10(noise_type=noise, severity=3,\n",
        "                                      num_samples=test_samples, train=False)\n",
        "        test_loader = DataLoader(test_ds, batch_size=32, shuffle=False,\n",
        "                                 num_workers=2, pin_memory=True)\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "        acc = 100. * correct / total\n",
        "        test_results[noise] = acc\n",
        "        print(f\"    {noise}: {acc:.2f}%\")\n",
        "\n",
        "    return test_results, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q29F5vI2BnK"
      },
      "source": [
        "üî¨ Experiments: Comparing Combinations\n",
        "\n",
        "We will run the following combinations:\n",
        "\n",
        "    Baseline: ReLU + LayerNorm\n",
        "\n",
        "    Original PASA: PASA + LayerNorm\n",
        "\n",
        "    Bayesian PASA: BayesianPASA + LayerNorm\n",
        "\n",
        "    Bayesian PASA + Bayesian R‚ÄëLayerNorm: BayesianPASA + BayesianRLayerNorm\n",
        "\n",
        "    Bayesian R‚ÄëLayerNorm alone: ReLU + BayesianRLayerNorm (to see its standalone effect)\n",
        "\n",
        "We'll store results and also collect softmax weights from the PASA models for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iCvSNJEMmgOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241497de-61e3-4425-9041-eac2112f3030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Training: norm=layer, act=relu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [00:03<00:00, 43.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching gaussian samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching shot_noise samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching blur samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching contrast samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1: Loss=2.2793, Acc=14.85%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2: Loss=2.2071, Acc=21.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3: Loss=2.1544, Acc=27.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4: Loss=2.1045, Acc=31.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 5: Loss=2.0549, Acc=34.80%\n",
            "Caching gaussian samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    gaussian: 20.00%\n",
            "Caching shot_noise samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    shot_noise: 16.50%\n",
            "Caching blur samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    blur: 29.50%\n",
            "Caching contrast samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    contrast: 23.50%\n",
            "\n",
            ">>> Training: norm=layer, act=pasa\n",
            "Caching gaussian samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching shot_noise samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching blur samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching contrast samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1: Loss=2.2617, Acc=16.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2: Loss=2.1991, Acc=22.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3: Loss=2.1524, Acc=27.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4: Loss=2.1046, Acc=29.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 5: Loss=2.0576, Acc=33.55%\n",
            "Caching gaussian samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    gaussian: 17.50%\n",
            "Caching shot_noise samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    shot_noise: 13.50%\n",
            "Caching blur samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    blur: 26.50%\n",
            "Caching contrast samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    contrast: 26.50%\n",
            "Caching gaussian samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Training: norm=layer, act=bayesian_pasa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching gaussian samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching shot_noise samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching blur samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching contrast samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1: Loss=2.2754, Acc=14.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2: Loss=2.2170, Acc=19.90%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3: Loss=2.1781, Acc=25.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4: Loss=2.1343, Acc=29.05%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 5: Loss=2.0912, Acc=30.00%\n",
            "Caching gaussian samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    gaussian: 21.00%\n",
            "Caching shot_noise samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    shot_noise: 21.50%\n",
            "Caching blur samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    blur: 25.00%\n",
            "Caching contrast samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    contrast: 26.50%\n",
            "Caching gaussian samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Training: norm=bayesian_r_layer, act=bayesian_pasa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching gaussian samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching shot_noise samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching blur samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching contrast samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1: Loss=2.2682, Acc=15.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2: Loss=2.2070, Acc=21.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3: Loss=2.1616, Acc=26.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4: Loss=2.1202, Acc=30.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 5: Loss=2.0731, Acc=32.70%\n",
            "Caching gaussian samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    gaussian: 21.50%\n",
            "Caching shot_noise samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    shot_noise: 18.50%\n",
            "Caching blur samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    blur: 27.00%\n",
            "Caching contrast samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    contrast: 27.50%\n",
            "Caching gaussian samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Training: norm=bayesian_r_layer, act=relu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching gaussian samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching shot_noise samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching blur samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching contrast samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1: Loss=2.2788, Acc=15.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 2: Loss=2.2054, Acc=22.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 3: Loss=2.1546, Acc=26.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 4: Loss=2.1065, Acc=36.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 5: Loss=2.0577, Acc=36.60%\n",
            "Caching gaussian samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    gaussian: 18.00%\n",
            "Caching shot_noise samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    shot_noise: 19.00%\n",
            "Caching blur samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    blur: 31.00%\n",
            "Caching contrast samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    contrast: 27.00%\n"
          ]
        }
      ],
      "source": [
        "# Define experiment configurations\n",
        "experiments = [\n",
        "    ('layer', 'relu', 'ReLU + LayerNorm'),\n",
        "    ('layer', 'pasa', 'PASA + LayerNorm'),\n",
        "    ('layer', 'bayesian_pasa', 'BayesianPASA + LayerNorm'),\n",
        "    ('bayesian_r_layer', 'bayesian_pasa', 'BayesianPASA + Bayesian R‚ÄëLayerNorm'),\n",
        "    ('bayesian_r_layer', 'relu', 'ReLU + Bayesian R‚ÄëLayerNorm'),\n",
        "]\n",
        "\n",
        "results = {}\n",
        "models = {}\n",
        "weights_data = {}  # to store softmax weights for PASA models\n",
        "\n",
        "# Run each experiment (reduce samples for quick demo; increase for final results)\n",
        "for norm, act, label in experiments:\n",
        "    test_acc, model = train_and_evaluate(norm, act,\n",
        "                                         num_epochs=5,          # increase to 10-20 for better accuracy\n",
        "                                         train_samples=500,    # reduce to 200 for speed\n",
        "                                         test_samples=200)\n",
        "    results[label] = test_acc\n",
        "    models[label] = model.cpu()  # move to CPU to save GPU memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # If this is a PASA model, collect weights on a sample batch\n",
        "    if 'pasa' in act:\n",
        "        # Create a small test loader for Gaussian noise\n",
        "        ds = CachedNoisyCIFAR10(noise_type='gaussian', severity=3,\n",
        "                                 num_samples=100, train=False)\n",
        "        loader = DataLoader(ds, batch_size=32, shuffle=False)\n",
        "        model.eval()\n",
        "        model.to(device)\n",
        "        all_weights = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, _ in loader:\n",
        "                inputs = inputs.to(device)\n",
        "                _, w = model(inputs, return_weights=True)\n",
        "                all_weights.append(w.cpu())\n",
        "        model.cpu()\n",
        "        torch.cuda.empty_cache()\n",
        "        weights_data[label] = torch.cat(all_weights, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qArjt5AT2HmP"
      },
      "source": [
        "üìä Visualization\n",
        "\n",
        "Test Accuracy Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GpEsGXLCmham"
      },
      "outputs": [],
      "source": [
        "# Prepare data for plotting\n",
        "noise_types = ['gaussian', 'shot_noise', 'blur', 'contrast']\n",
        "df_rows = []\n",
        "for label, acc_dict in results.items():\n",
        "    for noise in noise_types:\n",
        "        df_rows.append({'Model': label, 'Noise': noise, 'Accuracy': acc_dict[noise]})\n",
        "df = pd.DataFrame(df_rows)\n",
        "\n",
        "# Grouped bar plot\n",
        "plt.figure(figsize=(14, 6))\n",
        "x = np.arange(len(noise_types))\n",
        "width = 0.15\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, len(experiments)))\n",
        "\n",
        "for i, label in enumerate([e[2] for e in experiments]):\n",
        "    accs = [df[(df.Model == label) & (df.Noise == noise)]['Accuracy'].values[0] for noise in noise_types]\n",
        "    plt.bar(x + i*width - 2*width, accs, width, label=label, color=colors[i])\n",
        "\n",
        "plt.xlabel('Noise Type')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.title('Activation + Normalization Comparison on CIFAR-10-C')\n",
        "plt.xticks(x, noise_types)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('comparison_bar.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qokixlqG2RQb"
      },
      "source": [
        "Softmax Weight Distribution (for PASA models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BG08vRO_2Rvt"
      },
      "outputs": [],
      "source": [
        "if weights_data:\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))  # Changed from (2,3) to (3,3)\n",
        "    branch_names = ['Sigmoid', 'Linear', 'Noise']\n",
        "    for row, (label, weights) in enumerate(weights_data.items()):\n",
        "        for b in range(3):\n",
        "            ax = axes[row, b]  # Now row goes 0,1,2 which is valid\n",
        "            ax.hist(weights[..., b].numpy().flatten(), bins=50, alpha=0.7)\n",
        "            ax.set_title(f'{label} ‚Äì {branch_names[b]}')\n",
        "            ax.set_xlim(0, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('pasa_weights.png', dpi=150)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLETtFhr2YTT"
      },
      "source": [
        "\n",
        "\n",
        "Summary Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tUVW2SkJ2ZAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e94cdc2-b7d4-4a0d-be6b-3fdb4eff92fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Final Test Accuracies (%)\n",
            "======================================================================\n",
            "Model                                    Gaussian     Shot     Blur Contrast\n",
            "ReLU + LayerNorm    20.00    16.50    29.50    23.50\n",
            "PASA + LayerNorm    17.50    13.50    26.50    26.50\n",
            "BayesianPASA + LayerNorm    21.00    21.50    25.00    26.50\n",
            "BayesianPASA + Bayesian R‚ÄëLayerNorm    21.50    18.50    27.00    27.50\n",
            "ReLU + Bayesian R‚ÄëLayerNorm    18.00    19.00    31.00    27.00\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Final Test Accuracies (%)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Model':<40} {'Gaussian':>8} {'Shot':>8} {'Blur':>8} {'Contrast':>8}\")\n",
        "for label in [e[2] for e in experiments]:\n",
        "    row = [label]\n",
        "    for noise in noise_types:\n",
        "        row.append(f\"{results[label][noise]:>8.2f}\")\n",
        "    print(\" \".join(row))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2FcBOPW2hQQ"
      },
      "source": [
        "üßπ Memory Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tPVd3Bb_2hqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e712943-237b-4b9e-8fea-8786cd1dfedc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ All experiments completed. GPU memory freed.\n"
          ]
        }
      ],
      "source": [
        "# Clear all remaining GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\\n‚úÖ All experiments completed. GPU memory freed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kn1Aa4kUXL47"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}